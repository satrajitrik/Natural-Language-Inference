{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('/Users/satrajitmaitra/Downloads/glove.6B/glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop = list(string.punctuation) + stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    premises = []\n",
    "    hypotheses = []\n",
    "    labels = []\n",
    "    for i in range(len(data)):\n",
    "        premises.append(data[i]['sentence1'].encode('utf-8'))\n",
    "        hypotheses.append(data[i]['sentence2'].encode('utf-8'))\n",
    "        labels.append(data[i]['gold_label'])\n",
    "\n",
    "    for i in range(len(premises)):\n",
    "        premises[i] = [j for j in word_tokenize(premises[i]) if j not in stop]\n",
    "\n",
    "    premise_encodings = []\n",
    "    for i in range(len(premises)):\n",
    "        encoding = [zeros(50)]*15\n",
    "        for j in range(len(premises[i])):\n",
    "            if embeddings_index.get(str(premises[i][j]).lower()) is not None:\n",
    "                encoding[j] = embeddings_index.get(str(premises[i][j]).lower())\n",
    "        premise_encodings.append(encoding)\n",
    "\n",
    "    for i in range(len(hypotheses)):\n",
    "        hypotheses[i] = [j for j in word_tokenize(hypotheses[i]) if j not in stop]\n",
    "\n",
    "    hypotheses_encodings = []\n",
    "    for i in range(len(hypotheses)):\n",
    "        encoding = [zeros(50)]*15\n",
    "        for j in range(len(hypotheses[i])):\n",
    "            if embeddings_index.get(str(hypotheses[i][j]).lower()) is not None:\n",
    "                encoding[j] = embeddings_index.get(str(hypotheses[i][j]).lower())\n",
    "        hypotheses_encodings.append(encoding)\n",
    "\n",
    "    x1 = premise_encodings\n",
    "    x1 = [array(x1[i]).flatten() for i in range(len(x1))]\n",
    "\n",
    "    x2 = hypotheses_encodings\n",
    "    x2 = [array(x2[i]).flatten() for i in range(len(x2))]\n",
    "\n",
    "    rounded_labels = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == 'entailment':\n",
    "            rounded_labels.append([1.0, 0.0, 0.0])\n",
    "        elif labels[i] == 'contradiction':\n",
    "            rounded_labels.append([0.0, 1.0, 0.0])\n",
    "        else:\n",
    "            rounded_labels.append([0.0, 0.0, 1.0])\n",
    "\n",
    "    y = rounded_labels\n",
    "    return x1, x2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train dataset and create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:22: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/satrajitmaitra/NLP_projects/train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "x1, x2, y = process_data(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test dataset and create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/satrajitmaitra/NLP_projects/test.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "test_x1, test_x2, test_y = process_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dev dataset and create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "with open('/Users/satrajitmaitra/NLP_projects/dev.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dev_x1, dev_x2, dev_y = process_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Hyper-parameters \n",
    "f_input_size = 750\n",
    "f_hidden_size = 500\n",
    "f_output_size = 100\n",
    "\n",
    "g_input_size = 200\n",
    "g_hidden_size = 50\n",
    "g_output_size = 3\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, f_input_size, f_hidden_size, f_output_size, g_input_size, g_hidden_size, g_output_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc11 = nn.Linear(f_input_size, f_hidden_size)\n",
    "        self.fc12 = nn.Linear(f_input_size, f_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc21 = nn.Linear(f_hidden_size, f_output_size)\n",
    "        self.fc22 = nn.Linear(f_hidden_size, f_output_size)\n",
    "        self.gc1 = nn.Linear(g_input_size, g_hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.gc2 = nn.Linear(g_hidden_size, g_output_size) \n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        outf1 = self.fc11(x1)\n",
    "        outf2 = self.fc12(x2)\n",
    "        outf1 = self.relu(outf1)\n",
    "        outf2 = self.relu(outf2)\n",
    "        outf1 = self.fc21(outf1)\n",
    "        outf2 = self.fc22(outf2)\n",
    "        outf = torch.cat([outf1, outf2], dim=0)\n",
    "        outg = self.gc1(outf)\n",
    "        outg = self.relu(outg)\n",
    "        outg = self.gc2(outg)\n",
    "        outg = self.softmax(outg)\n",
    "        return outg\n",
    "\n",
    "\n",
    "model = NeuralNet(f_input_size, f_hidden_size, f_output_size, g_input_size, g_hidden_size, g_output_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [500/10000], Loss: 0.2688\n",
      "Epoch [1/10], Step [1000/10000], Loss: 0.1879\n",
      "Epoch [1/10], Step [1500/10000], Loss: 0.1554\n",
      "Epoch [1/10], Step [2000/10000], Loss: 0.1940\n",
      "Epoch [1/10], Step [2500/10000], Loss: 0.2852\n",
      "Epoch [1/10], Step [3000/10000], Loss: 0.2125\n",
      "Epoch [1/10], Step [3500/10000], Loss: 0.2512\n",
      "Epoch [1/10], Step [4000/10000], Loss: 0.2394\n",
      "Epoch [1/10], Step [4500/10000], Loss: 0.2060\n",
      "Epoch [1/10], Step [5000/10000], Loss: 0.2383\n",
      "Epoch [1/10], Step [5500/10000], Loss: 0.1870\n",
      "Epoch [1/10], Step [6000/10000], Loss: 0.2166\n",
      "Epoch [1/10], Step [6500/10000], Loss: 0.1936\n",
      "Epoch [1/10], Step [7000/10000], Loss: 0.1455\n",
      "Epoch [1/10], Step [7500/10000], Loss: 0.2046\n",
      "Epoch [1/10], Step [8000/10000], Loss: 0.2650\n",
      "Epoch [1/10], Step [8500/10000], Loss: 0.2245\n",
      "Epoch [1/10], Step [9000/10000], Loss: 0.2436\n",
      "Epoch [1/10], Step [9500/10000], Loss: 0.2084\n",
      "Epoch [1/10], Step [10000/10000], Loss: 0.0725\n",
      "Epoch [2/10], Step [500/10000], Loss: 0.4042\n",
      "Epoch [2/10], Step [1000/10000], Loss: 0.2072\n",
      "Epoch [2/10], Step [1500/10000], Loss: 0.1536\n",
      "Epoch [2/10], Step [2000/10000], Loss: 0.2283\n",
      "Epoch [2/10], Step [2500/10000], Loss: 0.2641\n",
      "Epoch [2/10], Step [3000/10000], Loss: 0.1031\n",
      "Epoch [2/10], Step [3500/10000], Loss: 0.1969\n",
      "Epoch [2/10], Step [4000/10000], Loss: 0.2262\n",
      "Epoch [2/10], Step [4500/10000], Loss: 0.1846\n",
      "Epoch [2/10], Step [5000/10000], Loss: 0.2260\n",
      "Epoch [2/10], Step [5500/10000], Loss: 0.1315\n",
      "Epoch [2/10], Step [6000/10000], Loss: 0.1955\n",
      "Epoch [2/10], Step [6500/10000], Loss: 0.1711\n",
      "Epoch [2/10], Step [7000/10000], Loss: 0.0968\n",
      "Epoch [2/10], Step [7500/10000], Loss: 0.2326\n",
      "Epoch [2/10], Step [8000/10000], Loss: 0.2982\n",
      "Epoch [2/10], Step [8500/10000], Loss: 0.3246\n",
      "Epoch [2/10], Step [9000/10000], Loss: 0.2773\n",
      "Epoch [2/10], Step [9500/10000], Loss: 0.1801\n",
      "Epoch [2/10], Step [10000/10000], Loss: 0.0304\n",
      "Epoch [3/10], Step [500/10000], Loss: 0.4194\n",
      "Epoch [3/10], Step [1000/10000], Loss: 0.1326\n",
      "Epoch [3/10], Step [1500/10000], Loss: 0.1657\n",
      "Epoch [3/10], Step [2000/10000], Loss: 0.2222\n",
      "Epoch [3/10], Step [2500/10000], Loss: 0.2265\n",
      "Epoch [3/10], Step [3000/10000], Loss: 0.0363\n",
      "Epoch [3/10], Step [3500/10000], Loss: 0.1070\n",
      "Epoch [3/10], Step [4000/10000], Loss: 0.2100\n",
      "Epoch [3/10], Step [4500/10000], Loss: 0.1390\n",
      "Epoch [3/10], Step [5000/10000], Loss: 0.1594\n",
      "Epoch [3/10], Step [5500/10000], Loss: 0.0548\n",
      "Epoch [3/10], Step [6000/10000], Loss: 0.1948\n",
      "Epoch [3/10], Step [6500/10000], Loss: 0.1235\n",
      "Epoch [3/10], Step [7000/10000], Loss: 0.0906\n",
      "Epoch [3/10], Step [7500/10000], Loss: 0.2939\n",
      "Epoch [3/10], Step [8000/10000], Loss: 0.2540\n",
      "Epoch [3/10], Step [8500/10000], Loss: 0.3191\n",
      "Epoch [3/10], Step [9000/10000], Loss: 0.3700\n",
      "Epoch [3/10], Step [9500/10000], Loss: 0.1053\n",
      "Epoch [3/10], Step [10000/10000], Loss: 0.0105\n",
      "Epoch [4/10], Step [500/10000], Loss: 0.4478\n",
      "Epoch [4/10], Step [1000/10000], Loss: 0.0415\n",
      "Epoch [4/10], Step [1500/10000], Loss: 0.0768\n",
      "Epoch [4/10], Step [2000/10000], Loss: 0.2185\n",
      "Epoch [4/10], Step [2500/10000], Loss: 0.0167\n",
      "Epoch [4/10], Step [3000/10000], Loss: 0.0031\n",
      "Epoch [4/10], Step [3500/10000], Loss: 0.0341\n",
      "Epoch [4/10], Step [4000/10000], Loss: 0.1859\n",
      "Epoch [4/10], Step [4500/10000], Loss: 0.1359\n",
      "Epoch [4/10], Step [5000/10000], Loss: 0.1391\n",
      "Epoch [4/10], Step [5500/10000], Loss: 0.0276\n",
      "Epoch [4/10], Step [6000/10000], Loss: 0.0834\n",
      "Epoch [4/10], Step [6500/10000], Loss: 0.1273\n",
      "Epoch [4/10], Step [7000/10000], Loss: 0.0748\n",
      "Epoch [4/10], Step [7500/10000], Loss: 0.4133\n",
      "Epoch [4/10], Step [8000/10000], Loss: 0.1593\n",
      "Epoch [4/10], Step [8500/10000], Loss: 0.1663\n",
      "Epoch [4/10], Step [9000/10000], Loss: 0.2499\n",
      "Epoch [4/10], Step [9500/10000], Loss: 0.0405\n",
      "Epoch [4/10], Step [10000/10000], Loss: 0.0250\n",
      "Epoch [5/10], Step [500/10000], Loss: 0.5098\n",
      "Epoch [5/10], Step [1000/10000], Loss: 0.0141\n",
      "Epoch [5/10], Step [1500/10000], Loss: 0.0363\n",
      "Epoch [5/10], Step [2000/10000], Loss: 0.2260\n",
      "Epoch [5/10], Step [2500/10000], Loss: 0.0019\n",
      "Epoch [5/10], Step [3000/10000], Loss: 0.0013\n",
      "Epoch [5/10], Step [3500/10000], Loss: 0.0009\n",
      "Epoch [5/10], Step [4000/10000], Loss: 0.1480\n",
      "Epoch [5/10], Step [4500/10000], Loss: 0.0379\n",
      "Epoch [5/10], Step [5000/10000], Loss: 0.1089\n",
      "Epoch [5/10], Step [5500/10000], Loss: 0.0138\n",
      "Epoch [5/10], Step [6000/10000], Loss: 0.0030\n",
      "Epoch [5/10], Step [6500/10000], Loss: 0.0987\n",
      "Epoch [5/10], Step [7000/10000], Loss: 0.0661\n",
      "Epoch [5/10], Step [7500/10000], Loss: 0.5112\n",
      "Epoch [5/10], Step [8000/10000], Loss: 0.0352\n",
      "Epoch [5/10], Step [8500/10000], Loss: 0.0123\n",
      "Epoch [5/10], Step [9000/10000], Loss: 0.0282\n",
      "Epoch [5/10], Step [9500/10000], Loss: 0.0035\n",
      "Epoch [5/10], Step [10000/10000], Loss: 0.1106\n",
      "Epoch [6/10], Step [500/10000], Loss: 0.3786\n",
      "Epoch [6/10], Step [1000/10000], Loss: 0.0140\n",
      "Epoch [6/10], Step [1500/10000], Loss: 0.0227\n",
      "Epoch [6/10], Step [2000/10000], Loss: 0.1891\n",
      "Epoch [6/10], Step [2500/10000], Loss: 0.0002\n",
      "Epoch [6/10], Step [3000/10000], Loss: 0.0000\n",
      "Epoch [6/10], Step [3500/10000], Loss: 0.0032\n",
      "Epoch [6/10], Step [4000/10000], Loss: 0.0712\n",
      "Epoch [6/10], Step [4500/10000], Loss: 0.0180\n",
      "Epoch [6/10], Step [5000/10000], Loss: 0.0096\n",
      "Epoch [6/10], Step [5500/10000], Loss: 0.0030\n",
      "Epoch [6/10], Step [6000/10000], Loss: 0.0026\n",
      "Epoch [6/10], Step [6500/10000], Loss: 0.2228\n",
      "Epoch [6/10], Step [7000/10000], Loss: 0.0232\n",
      "Epoch [6/10], Step [7500/10000], Loss: 0.5156\n",
      "Epoch [6/10], Step [8000/10000], Loss: 0.0004\n",
      "Epoch [6/10], Step [8500/10000], Loss: 0.0003\n",
      "Epoch [6/10], Step [9000/10000], Loss: 0.0441\n",
      "Epoch [6/10], Step [9500/10000], Loss: 0.0000\n",
      "Epoch [6/10], Step [10000/10000], Loss: 0.0072\n",
      "Epoch [7/10], Step [500/10000], Loss: 0.5356\n",
      "Epoch [7/10], Step [1000/10000], Loss: 0.0059\n",
      "Epoch [7/10], Step [1500/10000], Loss: 0.0052\n",
      "Epoch [7/10], Step [2000/10000], Loss: 0.1159\n",
      "Epoch [7/10], Step [2500/10000], Loss: 0.0000\n",
      "Epoch [7/10], Step [3000/10000], Loss: 0.0000\n",
      "Epoch [7/10], Step [3500/10000], Loss: 0.0000\n",
      "Epoch [7/10], Step [4000/10000], Loss: 0.0059\n",
      "Epoch [7/10], Step [4500/10000], Loss: 0.0148\n",
      "Epoch [7/10], Step [5000/10000], Loss: 0.0105\n",
      "Epoch [7/10], Step [5500/10000], Loss: 0.0194\n",
      "Epoch [7/10], Step [6000/10000], Loss: 0.0282\n",
      "Epoch [7/10], Step [6500/10000], Loss: 0.1323\n",
      "Epoch [7/10], Step [7000/10000], Loss: 0.0623\n",
      "Epoch [7/10], Step [7500/10000], Loss: 0.5120\n",
      "Epoch [7/10], Step [8000/10000], Loss: 0.0000\n",
      "Epoch [7/10], Step [8500/10000], Loss: 0.0000\n",
      "Epoch [7/10], Step [9000/10000], Loss: 0.0256\n",
      "Epoch [7/10], Step [9500/10000], Loss: 0.0000\n",
      "Epoch [7/10], Step [10000/10000], Loss: 0.0001\n",
      "Epoch [8/10], Step [500/10000], Loss: 0.0011\n",
      "Epoch [8/10], Step [1000/10000], Loss: 0.0041\n",
      "Epoch [8/10], Step [1500/10000], Loss: 0.0353\n",
      "Epoch [8/10], Step [2000/10000], Loss: 0.1492\n",
      "Epoch [8/10], Step [2500/10000], Loss: 0.0022\n",
      "Epoch [8/10], Step [3000/10000], Loss: 0.0000\n",
      "Epoch [8/10], Step [3500/10000], Loss: 0.0000\n",
      "Epoch [8/10], Step [4000/10000], Loss: 0.0003\n",
      "Epoch [8/10], Step [4500/10000], Loss: 0.0292\n",
      "Epoch [8/10], Step [5000/10000], Loss: 0.0009\n",
      "Epoch [8/10], Step [5500/10000], Loss: 0.0000\n",
      "Epoch [8/10], Step [6000/10000], Loss: 0.0000\n",
      "Epoch [8/10], Step [6500/10000], Loss: 0.4129\n",
      "Epoch [8/10], Step [7000/10000], Loss: 0.0720\n",
      "Epoch [8/10], Step [7500/10000], Loss: 0.2911\n",
      "Epoch [8/10], Step [8000/10000], Loss: 0.0000\n",
      "Epoch [8/10], Step [8500/10000], Loss: 0.0000\n",
      "Epoch [8/10], Step [9000/10000], Loss: 0.0002\n",
      "Epoch [8/10], Step [9500/10000], Loss: 0.0000\n",
      "Epoch [8/10], Step [10000/10000], Loss: 0.0000\n",
      "Epoch [9/10], Step [500/10000], Loss: 0.0019\n",
      "Epoch [9/10], Step [1000/10000], Loss: 0.0007\n",
      "Epoch [9/10], Step [1500/10000], Loss: 0.0027\n",
      "Epoch [9/10], Step [2000/10000], Loss: 0.1513\n",
      "Epoch [9/10], Step [2500/10000], Loss: 0.0000\n",
      "Epoch [9/10], Step [3000/10000], Loss: 0.0001\n",
      "Epoch [9/10], Step [3500/10000], Loss: 0.0000\n",
      "Epoch [9/10], Step [4000/10000], Loss: 0.0007\n",
      "Epoch [9/10], Step [4500/10000], Loss: 0.0026\n",
      "Epoch [9/10], Step [5000/10000], Loss: 0.0002\n",
      "Epoch [9/10], Step [5500/10000], Loss: 0.0002\n",
      "Epoch [9/10], Step [6000/10000], Loss: 0.0000\n",
      "Epoch [9/10], Step [6500/10000], Loss: 0.1200\n",
      "Epoch [9/10], Step [7000/10000], Loss: 0.2333\n",
      "Epoch [9/10], Step [7500/10000], Loss: 0.4532\n",
      "Epoch [9/10], Step [8000/10000], Loss: 0.0000\n",
      "Epoch [9/10], Step [8500/10000], Loss: 0.0000\n",
      "Epoch [9/10], Step [9000/10000], Loss: 0.0000\n",
      "Epoch [9/10], Step [9500/10000], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [10000/10000], Loss: 0.0000\n",
      "Epoch [10/10], Step [500/10000], Loss: 0.0178\n",
      "Epoch [10/10], Step [1000/10000], Loss: 0.1959\n",
      "Epoch [10/10], Step [1500/10000], Loss: 0.1037\n",
      "Epoch [10/10], Step [2000/10000], Loss: 0.0263\n",
      "Epoch [10/10], Step [2500/10000], Loss: 0.0000\n",
      "Epoch [10/10], Step [3000/10000], Loss: 0.0000\n",
      "Epoch [10/10], Step [3500/10000], Loss: 0.0000\n",
      "Epoch [10/10], Step [4000/10000], Loss: 0.0005\n",
      "Epoch [10/10], Step [4500/10000], Loss: 0.0002\n",
      "Epoch [10/10], Step [5000/10000], Loss: 0.0001\n",
      "Epoch [10/10], Step [5500/10000], Loss: 0.0000\n",
      "Epoch [10/10], Step [6000/10000], Loss: 0.0000\n",
      "Epoch [10/10], Step [6500/10000], Loss: 0.1140\n",
      "Epoch [10/10], Step [7000/10000], Loss: 0.1307\n",
      "Epoch [10/10], Step [7500/10000], Loss: 0.0287\n",
      "Epoch [10/10], Step [8000/10000], Loss: 0.0000\n",
      "Epoch [10/10], Step [8500/10000], Loss: 0.0000\n",
      "Epoch [10/10], Step [9000/10000], Loss: 0.0010\n",
      "Epoch [10/10], Step [9500/10000], Loss: 0.0000\n",
      "Epoch [10/10], Step [10000/10000], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "total_step = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(10000):\n",
    "        outputg = model(torch.tensor(x1[i]).float(), torch.tensor(x2[i]).float())\n",
    "\n",
    "        loss = nn.functional.mse_loss(outputg, torch.tensor(y[i]))\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 500 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1000 test premises/hypotheses: 31.8 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 1000.0\n",
    "    for i in range(1000):\n",
    "        outputg = model(torch.tensor(x1[i]).float(), torch.tensor(x2[i]).float())\n",
    "        \n",
    "        predicted = torch.argmax(outputg.data)\n",
    "\n",
    "        correct += (predicted == torch.argmax(torch.tensor(test_y[i]))).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 1000 test premises/hypotheses: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 train premises/hypotheses: 88.32 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 10000.0\n",
    "    for i in range(10000):\n",
    "        outputg = model(torch.tensor(x1[i]).float(), torch.tensor(x2[i]).float())\n",
    "\n",
    "        predicted = torch.argmax(outputg.data)\n",
    "\n",
    "        correct += (predicted == torch.argmax(torch.tensor(y[i]))).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 train premises/hypotheses: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1000 dev premises/hypotheses: 34.7 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 1000.0\n",
    "    for i in range(1000):\n",
    "        outputg = model(torch.tensor(x1[i]).float(), torch.tensor(x2[i]).float())\n",
    "        \n",
    "        predicted = torch.argmax(outputg.data)\n",
    "\n",
    "        correct += (predicted == torch.argmax(torch.tensor(dev_y[i]))).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 1000 dev premises/hypotheses: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# epoch 10; lr = 0.0001\n",
    "\n",
    "# f_output = 100, g_hidden = 50 (Test: 31.8, train: 88.32, dev: 34.7) MSEloss\n",
    "# f_output = 100, g_hidden = 100 (Test: 31.5, train: 87.95, dev: 35.1) MSEloss\n",
    "# f_output = 300, g_hidden = 100 (Test: 31.6, train: 83.65, dev: 34.8) MSEloss\n",
    "# f_output = 300, g_hidden = 50 (Test: 30.7, train: 85.59, dev: 35.1) MSEloss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
